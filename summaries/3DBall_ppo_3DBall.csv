Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
12000,1.4189383,21.87238095238095,0.2740357,1.1883587536921028,1.1883587536921028,1.0
24000,1.4138007,22.746534653465346,0.097684234,1.2687746557322415,1.2687746557322415,1.0
36000,1.3962377,26.0744920993228,0.16106455,1.6115123485603935,1.6115123485603935,1.0
48000,1.377771,34.63690476190476,0.33619037,2.4637982162594443,2.4637982162594443,1.0
60000,1.3551245,45.75968992248062,0.5328868,3.5286825049755186,3.5286825049755186,1.0
72000,1.3442516,78.48,0.92312795,6.9353342461586,6.9353342461586,1.0
84000,1.3336272,140.64705882352942,1.2841283,12.997648759449229,12.997648759449229,1.0
96000,1.3183625,313.36842105263156,1.7783101,28.389478369763022,28.389478369763022,1.0
108000,1.310133,451.81481481481484,2.0876553,45.75556227895949,45.75556227895949,1.0
120000,1.2979968,734.0588235294117,2.6984103,69.94118735369514,69.94118735369514,1.0
132000,1.2904321,870.0714285714286,3.0389595,86.95001336506435,86.95001336506435,1.0
144000,1.2865692,880.0769230769231,3.650102,87.85385924119215,87.85385924119215,1.0
156000,1.2793678,985.9166666666666,4.004411,98.6000150044759,98.6000150044759,1.0
