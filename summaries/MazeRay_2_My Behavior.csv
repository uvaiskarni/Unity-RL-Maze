Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
1000,1.3776672,0.42491457,43.57142857142857,-1.0,-1.0,1.0
2000,1.3771095,0.25674817,36.241379310344826,-0.21428571428571427,-0.21428571428571427,1.0
3000,1.3667326,0.006427127,22.666666666666668,-0.725,-0.725,1.0
4000,1.3656881,-0.40754002,54.89473684210526,-0.3888888888888889,-0.3888888888888889,1.0
5000,1.3477978,-0.407773,33.758620689655174,-0.6206896551724138,-0.6206896551724138,1.0
6000,1.339524,-0.4278367,28.085714285714285,-0.37142857142857144,-0.37142857142857144,1.0
7000,1.3292822,-0.46131423,53.27777777777778,-0.3888888888888889,-0.3888888888888889,1.0
8000,1.3281091,-0.5523368,34.78260869565217,0.375,0.375,1.0
9000,1.3272067,-0.48454213,95.08333333333333,1.75,1.75,1.0
10000,1.2926266,-0.59229267,47.61904761904762,0.1,0.1,1.0
11000,1.2688876,-0.5378976,51.388888888888886,0.15789473684210525,0.15789473684210525,1.0
12000,1.2849605,-0.6271973,83.25,-1.0,-1.0,1.0
