Steps,Policy/Extrinsic Value Estimate,Policy/Entropy,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
30000,9.27255,1.3602649,3.4373238,0.256174,0.00028464006,2999.0,-168.4444210562441,-168.4444210562441,1.0
35000,8.99712,1.3570065,3.1599858,0.24922426,0.00028161606,None,-175.99997865781188,-175.99997865781188,1.0
40000,8.202236,1.3554704,3.1554334,0.25138476,0.00027854406,None,None,None,1.0
45000,7.353403,1.3464518,2.5665271,0.2488087,0.000275472,None,None,None,1.0
50000,6.2936378,1.3445601,2.3979354,0.25490433,0.0002724,None,None,None,1.0
55000,5.944243,1.3447894,1.6334162,0.24658997,0.000269328,None,None,None,1.0
60000,5.3949556,1.3425059,1.259703,0.25242913,0.00026625601,2999.0,-107.22220738501184,-107.22220738501184,1.0
65000,5.66854,1.3068632,1.122202,0.2509719,0.00026323204,None,-118.9999834112823,-118.9999834112823,1.0
70000,4.8772306,1.2889073,0.9049153,0.2528502,0.00026016004,None,None,None,1.0
75000,4.460488,1.278014,0.59335333,0.25070027,0.00025708805,None,None,None,1.0
80000,3.7420177,1.2623534,0.4322604,0.23931766,0.000254016,None,None,None,1.0
85000,3.3772583,1.2468511,0.33017412,0.24503262,0.000250944,None,None,None,1.0
90000,3.205631,1.2781857,0.26202404,0.24265255,0.000247872,2999.0,-38.666661045410564,-38.666661045410564,1.0
